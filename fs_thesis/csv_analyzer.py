"""
CSV Daten Analyzer fÃ¼r Masterarbeit - MIMIC-IV Edition
Analysiert alle CSV-Dateien einzeln (verschiedene Schemas)
"""

import duckdb
import pandas as pd
import glob
import os
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Konfiguration
CSV_ORDNER = "/Users/andrey/Code-Kitchen/fs-thesis/files/mimic-iv-3.1/hosp"  # Dein Pfad
OUTPUT_ORDNER = "analyse_ergebnisse"
DUCKDB_FILE = "mimic_analyse.db"

# Output-Ordner erstellen
os.makedirs(OUTPUT_ORDNER, exist_ok=True)

# Seaborn Style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

print("="*80)
print("MIMIC-IV DATEN ANALYZER - MASTERARBEIT")
print("="*80)
print(f"Analysiere Dateien in: {CSV_ORDNER}")
print(f"Zeitpunkt: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)

# DuckDB Connection
con = duckdb.connect(DUCKDB_FILE)

# 1. DATEI-ÃœBERSICHT
print("\nðŸ“ SCHRITT 1: Datei-Ãœbersicht")
print("-" * 80)

csv_files = sorted(glob.glob(f"{CSV_ORDNER}/*.csv"))
print(f"Gefundene CSV-Dateien: {len(csv_files)}\n")

datei_info = []
for file in csv_files:
    size_mb = os.path.getsize(file) / (1024 * 1024)
    datei_info.append({
        'Dateiname': os.path.basename(file),
        'GrÃ¶ÃŸe (MB)': round(size_mb, 2),
        'Pfad': file
    })

df_dateien = pd.DataFrame(datei_info)
print(df_dateien[['Dateiname', 'GrÃ¶ÃŸe (MB)']].to_string(index=False))
print(f"\nðŸ“Š GesamtgrÃ¶ÃŸe: {df_dateien['GrÃ¶ÃŸe (MB)'].sum():.2f} MB ({df_dateien['GrÃ¶ÃŸe (MB)'].sum()/1024:.2f} GB)")

# 2. DETAILLIERTE ANALYSE JEDER TABELLE
print("\nðŸ“Š SCHRITT 2: Detaillierte Tabellen-Analyse")
print("-" * 80)

tabellen_info = []

for idx, file in enumerate(csv_files, 1):
    tabellen_name = os.path.basename(file).replace('.csv', '')
    print(f"\n[{idx}/{len(csv_files)}] Analysiere: {tabellen_name}")
    
    try:
        # Zeilenanzahl
        zeilen = con.execute(f"SELECT COUNT(*) FROM read_csv_auto('{file}')").fetchone()[0]
        
        # Spalten-Info
        schema = con.execute(f"DESCRIBE SELECT * FROM read_csv_auto('{file}')").df()
        spalten = schema['column_name'].tolist()
        typen = schema['column_type'].tolist()
        
        # Sample Daten
        sample = con.execute(f"SELECT * FROM read_csv_auto('{file}') LIMIT 3").df()
        
        # Speichern
        tabellen_info.append({
            'Tabelle': tabellen_name,
            'Zeilen': zeilen,
            'Spalten': len(spalten),
            'GrÃ¶ÃŸe_MB': round(os.path.getsize(file) / (1024 * 1024), 2),
            'Schema': list(zip(spalten, typen)),
            'Sample': sample
        })
        
        print(f"  âœ“ {zeilen:,} Zeilen, {len(spalten)} Spalten")
        print(f"    Spalten: {', '.join(spalten[:5])}{'...' if len(spalten) > 5 else ''}")
        
    except Exception as e:
        print(f"  âš ï¸ Fehler: {str(e)[:100]}")
        tabellen_info.append({
            'Tabelle': tabellen_name,
            'Fehler': str(e)
        })

# 3. ZUSAMMENFASSUNG
print("\nðŸ“ˆ SCHRITT 3: Gesamtzusammenfassung")
print("-" * 80)

df_summary = pd.DataFrame([{
    'Tabelle': t['Tabelle'],
    'Zeilen': t.get('Zeilen', 0),
    'Spalten': t.get('Spalten', 0),
    'GrÃ¶ÃŸe (MB)': t.get('GrÃ¶ÃŸe_MB', 0)
} for t in tabellen_info if 'Zeilen' in t])

# Nach GrÃ¶ÃŸe sortieren
df_summary = df_summary.sort_values('GrÃ¶ÃŸe (MB)', ascending=False)
print(df_summary.to_string(index=False))

gesamt_zeilen = df_summary['Zeilen'].sum()
print(f"\nâœ… GESAMT: {gesamt_zeilen:,} Zeilen Ã¼ber alle Tabellen")
print(f"âœ… GrÃ¶ÃŸte Tabelle: {df_summary.iloc[0]['Tabelle']} ({df_summary.iloc[0]['Zeilen']:,} Zeilen)")

# 4. TABELLEN IN DUCKDB LADEN
print("\nðŸ’¾ SCHRITT 4: Tabellen in DuckDB laden")
print("-" * 80)

erfolgreich = 0
for info in tabellen_info:
    if 'Zeilen' not in info:
        continue
    
    tabellen_name = info['Tabelle']
    file_path = f"{CSV_ORDNER}/{tabellen_name}.csv"
    
    try:
        # Jede Tabelle einzeln laden
        con.execute(f"""
            CREATE OR REPLACE TABLE {tabellen_name} AS
            SELECT * FROM read_csv_auto('{file_path}')
        """)
        erfolgreich += 1
        print(f"  âœ“ {tabellen_name}: {info['Zeilen']:,} Zeilen")
        
    except Exception as e:
        print(f"  âš ï¸ {tabellen_name}: Fehler - {str(e)[:80]}")

print(f"\nâœ… {erfolgreich}/{len(tabellen_info)} Tabellen erfolgreich geladen")
print(f"âœ… Datenbank gespeichert: {DUCKDB_FILE}")

# 5. SCHEMA-ÃœBERSICHT SPEICHERN
print("\nðŸ“‹ SCHRITT 5: Schema-Dokumentation erstellen")
print("-" * 80)

schema_datei = f"{OUTPUT_ORDNER}/schema_dokumentation.txt"
with open(schema_datei, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("MIMIC-IV SCHEMA DOKUMENTATION\n")
    f.write(f"Erstellt am: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write("="*80 + "\n\n")
    
    for info in tabellen_info:
        if 'Schema' not in info:
            continue
            
        f.write(f"\n{'='*80}\n")
        f.write(f"Tabelle: {info['Tabelle']}\n")
        f.write(f"Zeilen: {info['Zeilen']:,}\n")
        f.write(f"Spalten: {info['Spalten']}\n")
        f.write(f"GrÃ¶ÃŸe: {info['GrÃ¶ÃŸe_MB']} MB\n")
        f.write(f"{'-'*80}\n")
        f.write("Schema:\n")
        for spalte, typ in info['Schema']:
            f.write(f"  - {spalte}: {typ}\n")
        
        f.write(f"\nBeispiel-Daten (erste 3 Zeilen):\n")
        f.write(info['Sample'].to_string())
        f.write("\n")

print(f"âœ… Schema-Dokumentation: {schema_datei}")

# 6. WICHTIGE TABELLEN IDENTIFIZIEREN
print("\nðŸŽ¯ SCHRITT 6: Wichtige Tabellen fÃ¼r ML/Analyse")
print("-" * 80)

kern_tabellen = {
    'patients': 'Patienten-Stammdaten',
    'admissions': 'Krankenhaus-Aufenthalte', 
    'labevents': 'Labor-Werte (sehr groÃŸ!)',
    'diagnoses_icd': 'Diagnosen',
    'prescriptions': 'Medikationen',
    'procedures_icd': 'Prozeduren'
}

print("\nWichtige Kern-Tabellen:")
for tabelle, beschreibung in kern_tabellen.items():
    info = next((t for t in tabellen_info if t['Tabelle'] == tabelle), None)
    if info and 'Zeilen' in info:
        print(f"  â€¢ {tabelle}: {beschreibung}")
        print(f"    â†’ {info['Zeilen']:,} Zeilen, {info['Spalten']} Spalten")

# 7. BEISPIEL-ABFRAGEN
print("\nðŸ’¡ SCHRITT 7: Beispiel-Abfragen fÃ¼r deine Masterarbeit")
print("-" * 80)

beispiele = """
# 1. Patienten-Ãœbersicht
patienten = con.execute(\"\"\"
    SELECT gender, COUNT(*) as anzahl 
    FROM patients 
    GROUP BY gender
\"\"\").df()

# 2. Aufenthalte pro Jahr
aufenthalte = con.execute(\"\"\"
    SELECT 
        EXTRACT(YEAR FROM admittime) as jahr,
        COUNT(*) as anzahl
    FROM admissions
    GROUP BY jahr
    ORDER BY jahr
\"\"\").df()

# 3. HÃ¤ufigste Diagnosen
diagnosen = con.execute(\"\"\"
    SELECT 
        d.long_title,
        COUNT(*) as anzahl
    FROM diagnoses_icd di
    JOIN d_icd_diagnoses d ON di.icd_code = d.icd_code
    GROUP BY d.long_title
    ORDER BY anzahl DESC
    LIMIT 10
\"\"\").df()

# 4. Daten fÃ¼r ML laden (mit Sampling!)
ml_data = con.execute(\"\"\"
    SELECT * FROM labevents 
    USING SAMPLE 1%  -- Nur 1% fÃ¼r schnelles Testen!
\"\"\").df()
"""

print(beispiele)

# 8. REPORT ERSTELLEN
print("\nðŸ“„ SCHRITT 8: Abschluss-Report")
print("-" * 80)

report_datei = f"{OUTPUT_ORDNER}/analyse_report.txt"
with open(report_datei, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("MIMIC-IV ANALYSE-REPORT\n")
    f.write(f"Erstellt am: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write("="*80 + "\n\n")
    
    f.write(f"Analysierte Dateien: {len(csv_files)}\n")
    f.write(f"GesamtgrÃ¶ÃŸe: {df_dateien['GrÃ¶ÃŸe (MB)'].sum():.2f} MB\n")
    f.write(f"Gesamt Zeilen: {gesamt_zeilen:,}\n\n")
    
    f.write("Top 10 grÃ¶ÃŸte Tabellen:\n")
    f.write(df_summary.head(10).to_string(index=False))
    f.write("\n\n")
    
    f.write("Kern-Tabellen fÃ¼r Analyse:\n")
    for tabelle, beschreibung in kern_tabellen.items():
        info = next((t for t in tabellen_info if t['Tabelle'] == tabelle), None)
        if info and 'Zeilen' in info:
            f.write(f"  â€¢ {tabelle}: {info['Zeilen']:,} Zeilen\n")

print(f"âœ… Report gespeichert: {report_datei}")

# 9. NÃ„CHSTE SCHRITTE
print("\n" + "="*80)
print("ðŸŽ‰ ANALYSE ABGESCHLOSSEN!")
print("="*80)
print("\nðŸ“‚ Generierte Dateien:")
print(f"  â€¢ DuckDB: {DUCKDB_FILE}")
print(f"  â€¢ Schema: {schema_datei}")
print(f"  â€¢ Report: {report_datei}")

print("\nðŸš€ NÃ¤chste Schritte fÃ¼r deine Masterarbeit:")
print("\n1. DATEN ERKUNDEN:")
print("   python")
print("   >>> import duckdb")
print("   >>> con = duckdb.connect('mimic_analyse.db')")
print("   >>> con.execute('SHOW TABLES').df()  # Alle Tabellen anzeigen")

print("\n2. KERN-DATEN ANALYSIEREN:")
print("   â€¢ patients: Demographie")
print("   â€¢ admissions: Aufenthalte & Outcomes")
print("   â€¢ labevents: Labor-Werte (158 Mio Zeilen!)")
print("   â€¢ diagnoses_icd: Diagnosen")

print("\n3. ML VORBEREITUNG:")
print("   â€¢ JOIN zwischen Tabellen Ã¼ber subject_id & hadm_id")
print("   â€¢ Feature Engineering aus Labor-Werten")
print("   â€¢ Sampling fÃ¼r schnelles Prototyping (USING SAMPLE X%)")

print("\nðŸ’¡ TIPP: labevents ist RIESIG (17.5 GB)")
print("   â†’ Nutze ALWAYS Sampling oder Filter!")
print("   â†’ Beispiel: WHERE itemid IN (50912, 50931)  -- nur bestimmte Tests")

print("\n" + "="*80)

# Connection schlieÃŸen
con.close()
print("âœ… Fertig! Du kannst jetzt mit deiner Analyse starten! ðŸš€")